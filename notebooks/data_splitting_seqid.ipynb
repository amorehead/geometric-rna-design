{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split Creation\n",
    "\n",
    "This notebook creates data splits used to evaluate gRNAde on biologically dissimilar clusters of RNAs.\n",
    "\n",
    "**Workflow:**\n",
    "1. Cluster RNA sample sequences into groups based on: \n",
    "    - Sequence identity -- CD-HIT (Fu et al., 2012) with identity threshold of 90%.\n",
    "    - Structural similarity -- qTMclust for efficiently applying US-align with similarity threshold 0.45 (*WIP*).\n",
    "2. Order the clusters based on some metric:\n",
    "    - Avg. of intra-sequence avg. RMSD among available structures\n",
    "    - Avg. of intra-sequence number of structures available\n",
    "3. Training, validation, and test splits become progressively harder.\n",
    "    - Top 100 samples from clusters with highest metric -- test set.\n",
    "    - Next 100 samples from clusters with highest metric -- validation set.\n",
    "    - All remaining samples -- training set.\n",
    "    - For clusters with >20 samples within them -- training set.\n",
    "    - Very large (> 1000 nts) or very small (< 10nts) RNAs -- training set.\n",
    "4. If any samples were not assigned clusters, append them to the training set.\n",
    "\n",
    "Note that we separate very large RNA samples (> 1000 nts) from clustering and directly add these to the training set, as it is unlikely that we want to redesign very large RNAs. Likewise for very short RNA samples (< 10 nts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List, Literal, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, InsetPosition, mark_inset\n",
    "import seaborn as sns\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "from src.data_utils import get_avg_rmsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_sequence_identity(\n",
    "        input_sequences, \n",
    "        identity_threshold = 0.9,\n",
    "        word_size = 2,\n",
    "        input_file = \"input\",\n",
    "        output_file = \"output\"\n",
    "    ):\n",
    "    # https://manpages.ubuntu.com/manpages/impish/man1/cd-hit-est.1.html\n",
    "        \n",
    "    # Write input sequences to the temporary input file\n",
    "    SeqIO.write(input_sequences, input_file, \"fasta\")\n",
    "\n",
    "    # Run CD-HIT-EST\n",
    "    cmd = [\n",
    "        \"cd-hit-est\",\n",
    "        \"-i\", input_file,\n",
    "        \"-o\", output_file,\n",
    "        \"-c\", str(identity_threshold), # Sequence identity threshold (e.g., 90%)\n",
    "        \"-n\", str(word_size),          # Word size for sequence comparisson, larger is better (default: 2)\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    # Read clustered sequences from the temporary output file\n",
    "    clustered_sequences = list(SeqIO.parse(output_file, \"fasta\"))\n",
    "\n",
    "    # Process the clustering output\n",
    "    seq_idx_to_cluster = {}\n",
    "    with open(output_file + \".clstr\", \"r\") as f:\n",
    "        current_cluster = None\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                current_cluster = int(line.strip().split(\" \")[1])\n",
    "            else:\n",
    "                sequence_id = int(line.split(\">\")[1].split(\"...\")[0])\n",
    "                seq_idx_to_cluster[sequence_id] = current_cluster\n",
    "\n",
    "    # Delete temporary files\n",
    "    os.remove(input_file)\n",
    "    os.remove(output_file)\n",
    "    os.remove(output_file + \".clstr\")\n",
    "\n",
    "    return clustered_sequences, seq_idx_to_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_qtmclust_cluster_file(file_path: str) -> List[List[Any]]:\n",
    "    clusters = {}\n",
    "    with open(file_path) as file:\n",
    "        for line in file:\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "            valid_columns = [col for col in columns if col]  # filter out any empty columns\n",
    "            # NOTE: the representative structure is the first (col=0) structure for a given cluster\n",
    "            cluster_repr = valid_columns[0]\n",
    "            clusters[cluster_repr] = valid_columns\n",
    "    return list(clusters.values())\n",
    "\n",
    "\n",
    "def run_qtmclust(\n",
    "    chain_dir: str,\n",
    "    chain_list_filepath: str,\n",
    "    qtmclust_exec_path: str,\n",
    "    output_cluster_filepath: Optional[str] = None,\n",
    "    tm_cluster_threshold: float = 0.45,\n",
    "    chain_ter_mode: Literal[0, 1, 2, 3] = 3,\n",
    "    chain_split_mode: Literal[0, 1, 2] = 0,\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "    # For more information on `chain_ter_mode` and `chain_split_mode`, please see:\n",
    "    # https://github.com/pylelab/USalign/blob/58b42af9d58436279c21b4f4074db87f072fcc21/qTMclust.cpp#L72\n",
    "    # and\n",
    "    # https://github.com/pylelab/USalign/blob/58b42af9d58436279c21b4f4074db87f072fcc21/qTMclust.cpp#L78\n",
    "    cmd = [\n",
    "        qtmclust_exec_path,\n",
    "        \"-dir\",\n",
    "        (chain_dir if chain_dir.endswith(\"/\") else chain_dir + \"/\"),\n",
    "        chain_list_filepath,\n",
    "        \"-TMcut\",\n",
    "        str(tm_cluster_threshold),\n",
    "        \"-ter\",\n",
    "        str(chain_ter_mode),\n",
    "        \"-split\",\n",
    "        str(chain_split_mode),\n",
    "    ]\n",
    "    if output_cluster_filepath is not None:\n",
    "        cmd += [\"-o\", output_cluster_filepath]\n",
    "    subprocess.run(\" \".join(cmd), capture_output=True, shell=True)  # nosec\n",
    "    if output_cluster_filepath is not None:\n",
    "        output_clusters = parse_qtmclust_cluster_file(output_cluster_filepath)\n",
    "        output_clusters_df = pd.DataFrame(output_clusters)\n",
    "        return output_clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_structure_similarity(\n",
    "    input_pdb_files, \n",
    "    similarity_threshold = 0.45,\n",
    "    qtmclust_exec_path: str = \"~/Programs/USalign/qTMclust\",\n",
    "):\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"structsim_temp_dir_\")\n",
    "    chain_list_filepath = os.path.join(temp_dir, \"chain_list\")\n",
    "    with open(chain_list_filepath, \"w\") as f:\n",
    "        for pdb_file_index, pdb_file in enumerate(input_pdb_files):\n",
    "            # record the name of each PDB file in a temporary text file input\n",
    "            sample_name_without_extension = os.path.basename(os.path.splitext(pdb_file)[0])\n",
    "            sample_name_postfix = \"\" if pdb_file_index == (len(input_pdb_files) - 1) else \"\\n\"\n",
    "            f.write(f\"{sample_name_without_extension}{sample_name_postfix}\")\n",
    "    output_cluster_filepath = os.path.join(temp_dir, \"cluster.txt\")\n",
    "\n",
    "    clustered_structures_df = run_qtmclust(\n",
    "        chain_dir=temp_dir,\n",
    "        chain_list_filepath=chain_list_filepath,\n",
    "        qtmclust_exec_path=qtmclust_exec_path,\n",
    "        output_cluster_filepath=output_cluster_filepath,\n",
    "        tm_cluster_threshold=similarity_threshold,  # note: clusters two chains if their TM-score is `similarity_threshold` or greater\n",
    "        chain_ter_mode=0,  # note: reads all chains\n",
    "        chain_split_mode=0,  # note: parses all chains in a complex as a single chain\n",
    "    )\n",
    "    return clustered_structures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data list\n",
    "data_list = torch.load(os.path.join(\"../data/\", \"processed.pt\"))\n",
    "print(len(data_list))\n",
    "\n",
    "# List of sample sequences (used to create .fasta input file)\n",
    "seq_list = []\n",
    "for idx, data in enumerate(data_list):\n",
    "    seq = data[\"seq\"]\n",
    "    seq_list.append(SeqRecord(Seq(seq), id=str(idx)))  # the ID for each sequence is its index in data_list\n",
    "\n",
    "# List of intra-sequence avg. RMSDs\n",
    "rmsd_list = get_avg_rmsds(data_list)\n",
    "\n",
    "# List of number of structures per sequence\n",
    "count_list = [len(data[\"coords_list\"]) for data in data_list]\n",
    "\n",
    "assert len(data_list) == len(seq_list) == len(rmsd_list) == len(count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster at 80% sequence identity (lowest currently possible)\n",
    "clustered_sequences, seq_idx_to_cluster = create_clusters_sequence_identity(seq_list, identity_threshold=0.8, word_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "len(clustered_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: it seems very short sequences (<10nt) are not being clustered.\n",
    "# These will be added to the training set after initial splitting.\n",
    "try:\n",
    "    # Why does this fail? Guess: sequences are too short?\n",
    "    assert len(seq_idx_to_cluster.keys()) == len(seq_list)\n",
    "except:\n",
    "    # Which sequence indices are not clustered? What are their corresponding sequences?\n",
    "    idx_not_clustered = list(set(list(range(len(data_list)))) - set(seq_idx_to_cluster.keys()))\n",
    "    print(\"Number of missing indices after clustering: \", len(idx_not_clustered))\n",
    "    \n",
    "    seq_lens = []\n",
    "    for idx in idx_not_clustered:\n",
    "        seq_lens.append(len(data_list[idx][\"seq\"]))\n",
    "    print(\"Sequence lengths for missing indices:\")\n",
    "    print(f\"    Distribution: {np.mean(seq_lens)} +- {np.std(seq_lens)}\")\n",
    "    print(f\"    Max: {np.max(seq_lens)}, Min: {np.min(seq_lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_idx_to_cluster: (index in data_list: cluster ID)\n",
    "# (NEW) cluster_to_seq_idx_list: (cluster ID: list of indices in data_list)\n",
    "cluster_to_seq_idx_list = {}\n",
    "for seq_idx, cluster in seq_idx_to_cluster.items():\n",
    "    # Sanity check to filter very large or very small RNAs\n",
    "    if len(seq_list[seq_idx]) > 1000 or len(seq_list[seq_idx]) < 10 and seq_idx not in idx_not_clustered:\n",
    "        idx_not_clustered.append(seq_idx)\n",
    "        # print(f\"Pruned idx {seq_idx} of length {len(seq_list[seq_idx])}.\")\n",
    "    else:\n",
    "        if cluster in cluster_to_seq_idx_list.keys():\n",
    "            cluster_to_seq_idx_list[cluster].append(seq_idx)\n",
    "        else:\n",
    "            cluster_to_seq_idx_list[cluster] = [seq_idx]\n",
    "print(\"Number of unassigned indices (not clustered + too large + too small): \", len(idx_not_clustered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster sizes: number of sequences in each cluster\n",
    "cluster_ids = list(cluster_to_seq_idx_list.keys())\n",
    "cluster_sizes = [len(list) for list in cluster_to_seq_idx_list.values()]\n",
    "\n",
    "# Number of structures in each cluster (total and intra-sequence avg.)\n",
    "total_structs_list = []\n",
    "avg_structs_list = []\n",
    "avg_rmsds_list = []\n",
    "avg_seq_len_list = []\n",
    "for cluster, seq_idx_list in cluster_to_seq_idx_list.items():\n",
    "    count = []\n",
    "    rmsds = []\n",
    "    lens = []\n",
    "    for seq_idx in seq_idx_list:\n",
    "        count.append(count_list[seq_idx])\n",
    "        rmsds.append(rmsd_list[seq_idx])\n",
    "        lens.append(len(seq_list[seq_idx]))\n",
    "    total_structs_list.append(np.sum(count))\n",
    "    avg_structs_list.append(np.mean(count))\n",
    "    avg_rmsds_list.append(np.mean(rmsds))\n",
    "    avg_seq_len_list.append(np.mean(lens))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Cluster ID': cluster_ids,\n",
    "    'Cluster size': cluster_sizes,\n",
    "    'Total no. structures': total_structs_list,\n",
    "    'Avg. sequence length': avg_seq_len_list,\n",
    "    'Avg. intra-sequence no. structures': avg_structs_list,\n",
    "    'Avg. intra-sequence avg. RMSD': avg_rmsds_list,\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSD Split\n",
    "\n",
    "# Zip the two lists together\n",
    "zipped = zip(cluster_ids, avg_rmsds_list)\n",
    "# Sort the zipped list based on the values (descending order, highest first)\n",
    "sorted_zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "# Unzip the sorted list back into two separate lists\n",
    "sorted_cluster_ids, sorted_avg_rmsds_list = zip(*sorted_zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx_list = []\n",
    "val_idx_list = []\n",
    "train_idx_list = []\n",
    "\n",
    "for cluster in sorted_cluster_ids:\n",
    "    seq_idx_list = cluster_to_seq_idx_list[cluster]\n",
    "    cluster_size = len(seq_idx_list)\n",
    "\n",
    "    # Test set\n",
    "    if len(test_idx_list) < 100 and cluster_size < 25:\n",
    "        test_idx_list += seq_idx_list\n",
    "    \n",
    "    # Validation set\n",
    "    elif len(val_idx_list) < 100 and cluster_size < 25:\n",
    "        val_idx_list += seq_idx_list\n",
    "    \n",
    "    # Training set\n",
    "    else:\n",
    "        train_idx_list += seq_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the sequences that were not assigned any clusters into the training set\n",
    "try:\n",
    "    assert len(test_idx_list) + len(val_idx_list) + len(train_idx_list) == len(data_list)\n",
    "except:\n",
    "    train_idx_list += idx_not_clustered\n",
    "    assert len(test_idx_list) + len(val_idx_list) + len(train_idx_list) == len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((train_idx_list, val_idx_list, test_idx_list), \"../data/seqid_rmsd_split.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
